{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d72af3",
   "metadata": {},
   "source": [
    "# Instead of undersampling and near-miss sampling then logistic regression, we can also choose oversampling with SMOTE then fitting into logistical regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b49e7546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on test data : 0.9735495039368613\n",
      "ROC AUC score on test data : 0.9894235687711506\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df contains your original DataFrame with 'Class' as the target variable\n",
    "\n",
    "# Step 1: Split into features and target variable\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Step 2: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Perform oversampling using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "# Step 4: Split oversampled data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions on the test data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Step 7: Calculate precision and ROC-AUC score\n",
    "precision_pf = precision_score(y_test, y_pred)\n",
    "roc_auc_pf = roc_auc_score(y_test, log_reg.predict_proba(X_test)[:, 1])  # Extract probabilities for the positive class\n",
    "\n",
    "print(\"Precision on test data :\", precision_pf)\n",
    "print(\"ROC AUC score on test data :\", roc_auc_pf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7575a18",
   "metadata": {},
   "source": [
    "#### Thus we figure:\n",
    "1. Logistical regression is the best fit for our model, it can enhance any connectivity between low correlation variables and gives max ROC AUC score of .98\n",
    "2. t-SNE is the best method to visualise data since it reduces dimensions while preserving local clustering\n",
    "3. Sampling can be done two ways- before dividing into train/test or after dividing just on the train variables. We choose to do the former since the dataset is too skewed in the beginning so it is better to divide it into equal distributions-otherwise test predictions may be skewly highed since most values are 0 anyways. We also tried the Near Miss Sampling but it reduced the precision by the nature of how it works, so we chose to follow the pipelines and compare the roc-aucs: \n",
    "\n",
    "First pipeline- undersampling, logistical regression- .97 roc-auc\n",
    "\n",
    "Second pipeline- oversampling with SMOTE, logistical regression- .98 ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffe3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
